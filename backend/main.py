from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import chromadb
from chromadb.config import Settings
import hashlib
from datetime import datetime
import os
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å– LLM é…ç½®
LLM_API_KEY = os.getenv("LLM_API_KEY", "")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-3.5-turbo")

# åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
llm_client = None
if LLM_API_KEY:
    llm_client = OpenAI(
        api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL
    )

# åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)

# åˆ›å»º FastAPI åº”ç”¨å®ä¾‹
app = FastAPI(title="Web-Retrace API", version="3.0.0")

# é…ç½® CORS - å…è®¸ Chrome æ‰©å±•è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯å’Œé›†åˆ
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(
    name="web_pages",
    metadata={"description": "Stores web page content for RAG retrieval"}
)

# å®šä¹‰è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str

class MemorizeRequest(BaseModel):
    title: str
    content: str

# å®šä¹‰å“åº”æ¨¡å‹
class ChatResponse(BaseModel):
    response: str
    status: str

class MemorizeResponse(BaseModel):
    status: str
    doc_id: str
    title: str
    message: str

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - å¥åº·æ£€æŸ¥"""
    # è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
    count = collection.count()
    return {
        "message": "Web-Retrace API æ­£åœ¨è¿è¡Œ",
        "version": "2.0.0",
        "stored_pages": count
    }

@app.post("/memorize", response_model=MemorizeResponse)
async def memorize(request: MemorizeRequest):
    """
    è®°å¿†ç«¯ç‚¹ - ä½¿ç”¨æ–‡æœ¬åˆ†å—å­˜å‚¨ç½‘é¡µå†…å®¹åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        request: åŒ…å«é¡µé¢æ ‡é¢˜å’Œå†…å®¹çš„è¯·æ±‚ä½“
    
    Returns:
        åŒ…å«å­˜å‚¨çŠ¶æ€å’Œæ–‡æ¡£IDçš„å“åº”ä½“
    """
    try:
        # ç”Ÿæˆå”¯ä¸€çš„æºæ–‡æ¡£IDï¼ˆä½¿ç”¨æ ‡é¢˜+æ—¶é—´æˆ³çš„å“ˆå¸Œï¼‰
        timestamp = datetime.now().isoformat()
        source_id = hashlib.md5(f"{request.title}{timestamp}".encode()).hexdigest()
        
        # ä½¿ç”¨æ–‡æœ¬åˆ†å—å™¨æ‹†åˆ†å†…å®¹
        chunks = text_splitter.split_text(request.content)
        
        # å‡†å¤‡æ‰¹é‡å­˜å‚¨æ•°æ®
        chunk_ids = []
        chunk_documents = []
        chunk_metadatas = []
        
        for i, chunk in enumerate(chunks):
            # ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå”¯ä¸€ ID
            chunk_id = f"{source_id}_chunk_{i}"
            chunk_ids.append(chunk_id)
            chunk_documents.append(chunk)
            
            # ä¸ºæ¯ä¸ª chunk æ·»åŠ å®Œæ•´çš„å…ƒæ•°æ®
            chunk_metadatas.append({
                "title": request.title,
                "source_id": source_id,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "timestamp": timestamp
            })
        
        # æ‰¹é‡å­˜å‚¨æ‰€æœ‰ chunks åˆ° ChromaDB
        collection.add(
            documents=chunk_documents,
            metadatas=chunk_metadatas,
            ids=chunk_ids
        )
        
        return MemorizeResponse(
            status="success",
            doc_id=source_id,
            title=request.title,
            message=f"æˆåŠŸå­˜å‚¨é¡µé¢: {request.title} (æ‹†åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—)"
        )
    
    except Exception as e:
        return MemorizeResponse(
            status="error",
            doc_id="",
            title=request.title,
            message=f"å­˜å‚¨å¤±è´¥: {str(e)}"
        )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    èŠå¤©ç«¯ç‚¹ - ä½¿ç”¨ LLM ç”Ÿæˆæ™ºèƒ½å›ç­”ï¼ˆåŸºäº RAG æ£€ç´¢ï¼‰
    
    Args:
        request: åŒ…å«ç”¨æˆ·æ¶ˆæ¯çš„è¯·æ±‚ä½“
    
    Returns:
        åŒ…å« LLM ç”Ÿæˆçš„å›ç­”å’ŒçŠ¶æ€çš„å“åº”ä½“
    """
    try:
        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰å†…å®¹
        count = collection.count()
        
        if count == 0:
            # æ•°æ®åº“ä¸ºç©ºï¼Œè¿”å›ç®€å•å“åº”
            response_text = f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š{request.message}\n\nğŸ’¡ æç¤ºï¼šç›®å‰è¿˜æ²¡æœ‰è®°å¿†ä»»ä½•é¡µé¢ã€‚ç‚¹å‡»ã€ŒMemorize This Pageã€æŒ‰é’®æ¥ä¿å­˜é¡µé¢å†…å®¹ã€‚"
            return ChatResponse(
                response=response_text,
                status="success"
            )
        
        # ä½¿ç”¨ RAG æ£€ç´¢ Top 5 æœ€ç›¸å…³çš„æ–‡æœ¬å—
        results = collection.query(
            query_texts=[request.message],
            n_results=min(15, count)
        )
        
        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›¸å…³å†…å®¹
        if not results or not results['documents'] or len(results['documents'][0]) == 0:
            response_text = f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š{request.message}\n\næœªæ‰¾åˆ°ç›¸å…³çš„é¡µé¢å†…å®¹ã€‚"
            return ChatResponse(
                response=response_text,
                status="success"
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_snippets = []
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
            title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
            chunk_index = metadata.get('chunk_index', 0)
            context_snippets.append(f"[ç‰‡æ®µ {i} - æ¥è‡ª: {title}, å— #{chunk_index}]\n{doc}")
        
        context = "\n\n".join(context_snippets)
        
        # å¦‚æœ LLM å®¢æˆ·ç«¯å¯ç”¨ï¼Œä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        if llm_client:
            try:
                # æ„å»ºç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯
                system_prompt = """You are a helpful assistant. Answer the user's question based ONLY on the following context snippets. 
If the answer is not in the context, say you don't know. 
Please answer in the same language as the user's question.
Be concise and accurate."""
                
                user_message = f"""Context:
{context}

Question: {request.message}"""
                
                # è°ƒç”¨ LLM API
                completion = llm_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_message}
                    ],
                    temperature=0.7,
                    max_tokens=500
                )
                
                # æå– LLM å›ç­”
                llm_response = completion.choices[0].message.content
                
                return ChatResponse(
                    response=llm_response,
                    status="success"
                )
                
            except Exception as llm_error:
                # LLM è°ƒç”¨å¤±è´¥ï¼Œé™çº§ä¸ºåŸºç¡€æ–‡æœ¬æ£€ç´¢
                fallback_response = f"âš ï¸ LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œä¸ºæ‚¨å±•ç¤ºç›¸å…³ç‰‡æ®µï¼š\n\n"
                for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
                    title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    snippet = doc[:150] + "..." if len(doc) > 150 else doc
                    fallback_response += f"ğŸ“„ {i}. {title}\n{snippet}\n\n"
                
                fallback_response += f"\nğŸ”§ é”™è¯¯è¯¦æƒ…: {str(llm_error)}"
                
                return ChatResponse(
                    response=fallback_response,
                    status="fallback"
                )
        else:
            # LLM å®¢æˆ·ç«¯æœªé…ç½®ï¼Œè¿”å›åŸºç¡€æ–‡æœ¬æ£€ç´¢ç»“æœ
            response_text = f"ğŸ’¡ æç¤ºï¼šè¯·é…ç½® LLM API Key ä»¥è·å¾—æ™ºèƒ½é—®ç­”åŠŸèƒ½ã€‚\n\næ ¹æ®æ‚¨çš„é—®é¢˜ã€Œ{request.message}ã€ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n"
            
            for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                snippet = doc[:150] + "..." if len(doc) > 150 else doc
                response_text += f"ğŸ“„ {i}. {title}\n{snippet}\n\n"
            
            return ChatResponse(
                response=response_text,
                status="success"
            )
    
    except Exception as e:
        return ChatResponse(
            response=f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}",
            status="error"
        )

if __name__ == "__main__":
    import uvicorn
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(app, host="127.0.0.1", port=8000, reload=True)
