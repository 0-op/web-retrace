from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import chromadb
from chromadb.config import Settings
import hashlib
from datetime import datetime
import os
import json
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# LLMå®¢æˆ·ç«¯ï¼ˆæ”¯æŒçƒ­é‡è½½ï¼‰
def get_llm_client():
    """è·å–LLMå®¢æˆ·ç«¯ï¼ˆæ¯æ¬¡é‡æ–°è¯»å–é…ç½®ï¼‰"""
    # é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(override=True)
    
    api_key = os.getenv("LLM_API_KEY", "")
    base_url = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
    
    if api_key:
        return OpenAI(api_key=api_key, base_url=base_url)
    return None

def get_llm_model():
    """è·å–å½“å‰é…ç½®çš„æ¨¡å‹åç§°"""
    load_dotenv(override=True)
    return os.getenv("LLM_MODEL", "gpt-3.5-turbo")

# åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)

# åˆ›å»º FastAPI åº”ç”¨å®ä¾‹
app = FastAPI(title="Web-Retrace API", version="3.0.0")

# é…ç½® CORS - å…è®¸ Chrome æ‰©å±•è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯å’Œé›†åˆ
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(
    name="web_pages",
    metadata={"description": "Stores web page content for RAG retrieval"}
)

# å®šä¹‰è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str

class MemorizeRequest(BaseModel):
    title: str
    content: str

# å®šä¹‰å“åº”æ¨¡å‹
class ChatResponse(BaseModel):
    response: str
    status: str

class MemorizeResponse(BaseModel):
    status: str
    doc_id: str
    title: str
    message: str

class SettingsRequest(BaseModel):
    api_key: str
    base_url: str = "https://api.siliconflow.cn/v1"
    model: str = "deepseek-ai/DeepSeek-V3"

# è¾…åŠ©å‡½æ•°ï¼šè¯»å†™.envæ–‡ä»¶
def read_env_file():
    """è¯»å–.envæ–‡ä»¶å†…å®¹"""
    env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
    env_vars = {}
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    env_vars[key] = value
    return env_vars

def write_env_file(settings):
    """å†™å…¥è®¾ç½®åˆ°.envæ–‡ä»¶"""
    env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
    with open(env_path, 'w', encoding='utf-8') as f:
        f.write(f"# LLM API Configuration\n")
        f.write(f"# ç”±æ¡Œé¢å®¢æˆ·ç«¯è‡ªåŠ¨ç”Ÿæˆ\n\n")
        f.write(f"LLM_API_KEY={settings['api_key']}\n")
        f.write(f"LLM_BASE_URL={settings['base_url']}\n")
        f.write(f"LLM_MODEL={settings['model']}\n")

# å¤šé…ç½®ç®¡ç†
def get_configs_file_path():
    """è·å–é…ç½®æ–‡ä»¶è·¯å¾„"""
    return os.path.join(os.path.dirname(__file__), '..', 'api_configs.json')

def read_all_configs():
    """è¯»å–æ‰€æœ‰APIé…ç½®"""
    config_path = get_configs_file_path()
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"configs": [], "active_config_id": None}

def write_all_configs(data):
    """ä¿å­˜æ‰€æœ‰é…ç½®åˆ°JSONæ–‡ä»¶"""
    config_path = get_configs_file_path()
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # åŒæ—¶æ›´æ–°.envæ–‡ä»¶ä¸ºå½“å‰æ¿€æ´»çš„é…ç½®
    if data.get('active_config_id') and data.get('configs'):
        active_config = next(
            (c for c in data['configs'] if c['id'] == data['active_config_id']),
            None
        )
        if active_config:
            write_env_file({
                'api_key': active_config.get('apiKey', ''),
                'base_url': active_config.get('baseUrl', ''),
                'model': active_config.get('model', '')
            })



@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - å¥åº·æ£€æŸ¥"""
    # è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
    count = collection.count()
    return {
        "message": "Web-Retrace API æ­£åœ¨è¿è¡Œ",
        "version": "2.0.0",
        "stored_pages": count
    }

@app.post("/memorize", response_model=MemorizeResponse)
async def memorize(request: MemorizeRequest):
    """
    è®°å¿†ç«¯ç‚¹ - ä½¿ç”¨æ–‡æœ¬åˆ†å—å­˜å‚¨ç½‘é¡µå†…å®¹åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        request: åŒ…å«é¡µé¢æ ‡é¢˜å’Œå†…å®¹çš„è¯·æ±‚ä½“
    
    Returns:
        åŒ…å«å­˜å‚¨çŠ¶æ€å’Œæ–‡æ¡£IDçš„å“åº”ä½“
    """
    try:
        # ç”Ÿæˆå”¯ä¸€çš„æºæ–‡æ¡£IDï¼ˆä½¿ç”¨æ ‡é¢˜+æ—¶é—´æˆ³çš„å“ˆå¸Œï¼‰
        timestamp = datetime.now().isoformat()
        source_id = hashlib.md5(f"{request.title}{timestamp}".encode()).hexdigest()
        
        # ä½¿ç”¨æ–‡æœ¬åˆ†å—å™¨æ‹†åˆ†å†…å®¹
        chunks = text_splitter.split_text(request.content)
        
        # å‡†å¤‡æ‰¹é‡å­˜å‚¨æ•°æ®
        chunk_ids = []
        chunk_documents = []
        chunk_metadatas = []
        
        for i, chunk in enumerate(chunks):
            # ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå”¯ä¸€ ID
            chunk_id = f"{source_id}_chunk_{i}"
            chunk_ids.append(chunk_id)
            chunk_documents.append(chunk)
            
            # ä¸ºæ¯ä¸ª chunk æ·»åŠ å®Œæ•´çš„å…ƒæ•°æ®
            chunk_metadatas.append({
                "title": request.title,
                "source_id": source_id,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "timestamp": timestamp
            })
        
        # æ‰¹é‡å­˜å‚¨æ‰€æœ‰ chunks åˆ° ChromaDB
        collection.add(
            documents=chunk_documents,
            metadatas=chunk_metadatas,
            ids=chunk_ids
        )
        
        return MemorizeResponse(
            status="success",
            doc_id=source_id,
            title=request.title,
            message=f"æˆåŠŸå­˜å‚¨é¡µé¢: {request.title} (æ‹†åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—)"
        )
    
    except Exception as e:
        return MemorizeResponse(
            status="error",
            doc_id="",
            title=request.title,
            message=f"å­˜å‚¨å¤±è´¥: {str(e)}"
        )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    èŠå¤©ç«¯ç‚¹ - ä½¿ç”¨ LLM ç”Ÿæˆæ™ºèƒ½å›ç­”ï¼ˆåŸºäº RAG æ£€ç´¢ï¼‰
    
    Args:
        request: åŒ…å«ç”¨æˆ·æ¶ˆæ¯çš„è¯·æ±‚ä½“
    
    Returns:
        åŒ…å« LLM ç”Ÿæˆçš„å›ç­”å’ŒçŠ¶æ€çš„å“åº”ä½“
    """
    try:
        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰å†…å®¹
        count = collection.count()
        
        if count == 0:
            # æ•°æ®åº“ä¸ºç©ºï¼Œè¿”å›ç®€å•å“åº”
            response_text = f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š{request.message}\n\nğŸ’¡ æç¤ºï¼šç›®å‰è¿˜æ²¡æœ‰è®°å¿†ä»»ä½•é¡µé¢ã€‚ç‚¹å‡»ã€ŒMemorize This Pageã€æŒ‰é’®æ¥ä¿å­˜é¡µé¢å†…å®¹ã€‚"
            return ChatResponse(
                response=response_text,
                status="success"
            )
        
        # ä½¿ç”¨ RAG æ£€ç´¢ Top 5 æœ€ç›¸å…³çš„æ–‡æœ¬å—
        results = collection.query(
            query_texts=[request.message],
            n_results=min(15, count)
        )
        
        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›¸å…³å†…å®¹
        if not results or not results['documents'] or len(results['documents'][0]) == 0:
            response_text = f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š{request.message}\n\næœªæ‰¾åˆ°ç›¸å…³çš„é¡µé¢å†…å®¹ã€‚"
            return ChatResponse(
                response=response_text,
                status="success"
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_snippets = []
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
            title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
            chunk_index = metadata.get('chunk_index', 0)
            context_snippets.append(f"[ç‰‡æ®µ {i} - æ¥è‡ª: {title}, å— #{chunk_index}]\n{doc}")
        
        context = "\n\n".join(context_snippets)
        
        # å¦‚æœ LLM å®¢æˆ·ç«¯å¯ç”¨ï¼Œä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        llm_client = get_llm_client()
        if llm_client:
            try:
                # æ„å»ºç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯
                system_prompt = """You are a helpful assistant. Answer the user's question based ONLY on the following context snippets. 
If the answer is not in the context, say you don't know. 
Please answer in the same language as the user's question.
Be concise and accurate."""
                
                user_message = f"""Context:
{context}

Question: {request.message}"""
                
                # è°ƒç”¨ LLM API
                completion = llm_client.chat.completions.create(
                    model=get_llm_model(),
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_message}
                    ],
                    temperature=0.7,
                    max_tokens=500
                )
                
                # æå– LLM å›ç­”
                llm_response = completion.choices[0].message.content
                
                return ChatResponse(
                    response=llm_response,
                    status="success"
                )
                
            except Exception as llm_error:
                # LLM è°ƒç”¨å¤±è´¥ï¼Œé™çº§ä¸ºåŸºç¡€æ–‡æœ¬æ£€ç´¢
                fallback_response = f"âš ï¸ LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œä¸ºæ‚¨å±•ç¤ºç›¸å…³ç‰‡æ®µï¼š\n\n"
                for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
                    title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    snippet = doc[:150] + "..." if len(doc) > 150 else doc
                    fallback_response += f"ğŸ“„ {i}. {title}\n{snippet}\n\n"
                
                fallback_response += f"\nğŸ”§ é”™è¯¯è¯¦æƒ…: {str(llm_error)}"
                
                return ChatResponse(
                    response=fallback_response,
                    status="fallback"
                )
        else:
            # LLM å®¢æˆ·ç«¯æœªé…ç½®ï¼Œè¿”å›åŸºç¡€æ–‡æœ¬æ£€ç´¢ç»“æœ
            response_text = f"ğŸ’¡ æç¤ºï¼šè¯·é…ç½® LLM API Key ä»¥è·å¾—æ™ºèƒ½é—®ç­”åŠŸèƒ½ã€‚\n\næ ¹æ®æ‚¨çš„é—®é¢˜ã€Œ{request.message}ã€ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n"
            
            for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                snippet = doc[:150] + "..." if len(doc) > 150 else doc
                response_text += f"ğŸ“„ {i}. {title}\n{snippet}\n\n"
            
            return ChatResponse(
                response=response_text,
                status="success"
            )
    
    except Exception as e:
        return ChatResponse(
            response=f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}",
            status="error"
        )

@app.post("/chat-free", response_model=ChatResponse)
async def chat_free(request: ChatRequest):
    """
    è‡ªç”±èŠå¤©ç«¯ç‚¹ - ç»“åˆçŸ¥è¯†åº“ä½†å…è®¸AIè‡ªç”±å‘æŒ¥
    
    Args:
        request: åŒ…å«ç”¨æˆ·æ¶ˆæ¯çš„è¯·æ±‚ä½“
    
    Returns:
        åŒ…å« LLM ç”Ÿæˆçš„å›ç­”å’ŒçŠ¶æ€çš„å“åº”ä½“
    """
    try:
        llm_client = get_llm_client()
        
        if not llm_client:
            return ChatResponse(
                response="âš ï¸ LLMæœªé…ç½®ï¼Œè¯·åœ¨æ¡Œé¢å®¢æˆ·ç«¯è®¾ç½®ä¸­é…ç½®API Key",
                status="error"
            )
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰å†…å®¹
        count = collection.count()
        context = ""
        
        if count > 0:
            # æ£€ç´¢ç›¸å…³å†…å®¹ä½œä¸ºå‚è€ƒ
            results = collection.query(
                query_texts=[request.message],
                n_results=min(5, count)
            )
            
            if results and results['documents'] and len(results['documents'][0]) > 0:
                context_snippets = []
                for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
                    title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    context_snippets.append(f"[å‚è€ƒ {i} - {title}]\n{doc}")
                context = "\n\n".join(context_snippets)
        
        # æ„å»ºè‡ªç”±æ¨¡å¼çš„ç³»ç»Ÿæç¤º
        system_prompt = """You are a helpful and knowledgeable assistant. 
If the user's question relates to the provided context, use it as a reference.
However, you are NOT limited to the context - you can also use your general knowledge to provide a comprehensive answer.
If the context is empty or irrelevant, just answer based on your general knowledge.
Please answer in the same language as the user's question.
Be helpful, accurate, and engaging."""
        
        if context:
            user_message = f"""å‚è€ƒèµ„æ–™ï¼ˆå¦‚ç›¸å…³ï¼‰:
{context}

ç”¨æˆ·é—®é¢˜: {request.message}"""
        else:
            user_message = request.message
        
        # è°ƒç”¨ LLM API  
        completion = llm_client.chat.completions.create(
            model=get_llm_model(),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        llm_response = completion.choices[0].message.content
        
        return ChatResponse(
            response=llm_response,
            status="success"
        )
    
    except Exception as e:
        return ChatResponse(
            response=f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}",
            status="error"
        )


@app.get("/pages")
async def get_all_pages():
    """
    è·å–æ‰€æœ‰å·²ä¿å­˜é¡µé¢çš„åˆ—è¡¨
    
    Returns:
        åŒ…å«é¡µé¢åˆ—è¡¨çš„å“åº”
    """
    try:
        # è·å–æ‰€æœ‰æ–‡æ¡£
        results = collection.get()
        
        if not results or not results['documents']:
            return {"pages": [], "total": 0}
        
        # æŒ‰source_idåˆ†ç»„ç»Ÿè®¡
        pages_dict = {}
        for i, (doc, metadata) in enumerate(zip(results['documents'], results['metadatas'])):
            source_id = metadata.get('source_id', 'unknown')
            
            if source_id not in pages_dict:
                pages_dict[source_id] = {
                    'id': source_id,
                    'title': metadata.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                    'timestamp': metadata.get('timestamp', ''),
                    'chunks': 1,
                    'preview': doc[:200] if doc else ''
                }
            else:
                pages_dict[source_id]['chunks'] += 1
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¶é—´æ’åº
        pages = list(pages_dict.values())
        pages.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return {
            "pages": pages,
            "total": len(pages)
        }
    
    except Exception as e:
        return {
            "pages": [],
            "total": 0,
            "error": str(e)
        }

@app.get("/pages/{page_id}")
async def get_page_detail(page_id: str):
    """
    è·å–å•ä¸ªé¡µé¢çš„è¯¦ç»†å†…å®¹
    
    Args:
        page_id: é¡µé¢çš„source_id
    
    Returns:
        åŒ…å«é¡µé¢è¯¦ç»†ä¿¡æ¯çš„å“åº”
    """
    try:
        # æŸ¥è¯¢è¯¥source_idçš„æ‰€æœ‰chunks
        results = collection.get(
            where={"source_id": page_id}
        )
        
        if not results or not results['documents']:
            return {
                "error": "é¡µé¢ä¸å­˜åœ¨",
                "id": page_id
            }
        
        # åˆå¹¶æ‰€æœ‰chunks
        metadata = results['metadatas'][0]
        content = '\n\n'.join(results['documents'])
        
        return {
            "id": page_id,
            "title": metadata.get('title', 'æœªçŸ¥æ ‡é¢˜'),
            "timestamp": metadata.get('timestamp', ''),
            "chunks": len(results['documents']),
            "content": content
        }
    
    except Exception as e:
        return {
            "error": str(e),
            "id": page_id
        }

@app.get("/settings")
async def get_settings():
    """è·å–å½“å‰LLMé…ç½®"""
    try:
        env_vars = read_env_file()
        return {
            "api_key": env_vars.get("LLM_API_KEY", ""),
            "base_url": env_vars.get("LLM_BASE_URL", "https://api.siliconflow.cn/v1"),
            "model": env_vars.get("LLM_MODEL", "deepseek-ai/DeepSeek-V3")
        }
    except Exception as e:
        return {
            "api_key": "",
            "base_url": "https://api.siliconflow.cn/v1",
            "model": "deepseek-ai/DeepSeek-V3",
            "error": str(e)
        }

@app.post("/settings")
async def save_settings(settings: SettingsRequest):
    """ä¿å­˜LLMé…ç½®åˆ°.envæ–‡ä»¶"""
    try:
        write_env_file({
            "api_key": settings.api_key,
            "base_url": settings.base_url,
            "model": settings.model
        })
        return {
            "status": "success",
            "message": "è®¾ç½®å·²ä¿å­˜ï¼Œè¯·é‡å¯åº”ç”¨ä»¥ç”Ÿæ•ˆ"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"ä¿å­˜å¤±è´¥: {str(e)}"
        }

@app.get("/api-configs")
async def get_api_configs():
    """è·å–æ‰€æœ‰APIé…ç½®"""
    try:
        return read_all_configs()
    except Exception as e:
        return {
            "configs": [],
            "active_config_id": None,
            "error": str(e)
        }

@app.post("/api-configs")
async def save_api_configs(data: dict):
    """ä¿å­˜æ‰€æœ‰APIé…ç½®"""
    try:
        write_all_configs(data)
        return {
            "status": "success",
            "message": "é…ç½®å·²ä¿å­˜"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"ä¿å­˜å¤±è´¥: {str(e)}"
        }


if __name__ == "__main__":
    import uvicorn
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(app, host="127.0.0.1", port=8000, reload=True)
